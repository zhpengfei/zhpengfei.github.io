<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="AI应用开发学习博客 - 深入学习微调、RAG、智能体等AI技术">
    <title>AI应用开发学习博客</title>
    <style>
        /* CSS变量系统 */
        :root {
            /* 颜色系统 */
            --color-primary: #2563eb;
            --color-primary-dark: #1e40af;
            --color-primary-light: #3b82f6;
            --color-text: #1f2937;
            --color-text-light: #6b7280;
            --color-background: #ffffff;
            --color-background-alt: #f9fafb;
            --color-border: #e5e7eb;
            --color-code-bg: #f3f4f6;
            --color-shadow: rgba(0, 0, 0, 0.1);
            
            /* 字体系统 */
            --font-sans: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            --font-mono: "SF Mono", Monaco, "Cascadia Code", "Courier New", monospace;
            
            /* 间距系统 */
            --spacing-xs: 0.25rem;
            --spacing-sm: 0.5rem;
            --spacing-md: 1rem;
            --spacing-lg: 1.5rem;
            --spacing-xl: 2rem;
            --spacing-2xl: 3rem;
            --spacing-3xl: 4rem;
        }

        /* 基础样式重置 */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: var(--font-sans);
            color: var(--color-text);
            background-color: var(--color-background);
            line-height: 1.6;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        /* 导航栏样式 */
        .nav {
            background-color: var(--color-background);
            border-bottom: 1px solid var(--color-border);
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 1px 3px var(--color-shadow);
        }

        .nav-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: var(--spacing-lg) var(--spacing-xl);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-title {
            font-size: 1.5rem;
            font-weight: 700;
            color: var(--color-primary);
            text-decoration: none;
            cursor: pointer;
            transition: color 0.2s;
        }

        .nav-title:hover {
            color: var(--color-primary-dark);
        }

        .nav-links {
            display: flex;
            gap: var(--spacing-lg);
        }

        .nav-link {
            color: var(--color-text-light);
            text-decoration: none;
            font-weight: 500;
            transition: color 0.2s;
            cursor: pointer;
        }

        .nav-link:hover {
            color: var(--color-primary);
        }

        /* 主容器 */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: var(--spacing-2xl) var(--spacing-xl);
        }

        /* 首页头部 */
        .hero {
            text-align: center;
            margin-bottom: var(--spacing-3xl);
        }

        .hero-title {
            font-size: 2.5rem;
            font-weight: 800;
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        .hero-description {
            font-size: 1.125rem;
            color: var(--color-text-light);
            max-width: 600px;
            margin: 0 auto;
        }

        /* 文章列表网格 */
        .articles-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(320px, 1fr));
            gap: var(--spacing-xl);
            margin-top: var(--spacing-2xl);
        }

        /* 文章卡片 */
        .article-card {
            background: var(--color-background);
            border: 1px solid var(--color-border);
            border-radius: 0.5rem;
            padding: var(--spacing-xl);
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 1px 3px var(--color-shadow);
        }

        .article-card:hover {
            transform: translateY(-4px);
            box-shadow: 0 4px 12px var(--color-shadow);
            border-color: var(--color-primary-light);
        }

        .article-meta {
            display: flex;
            align-items: center;
            gap: var(--spacing-md);
            margin-bottom: var(--spacing-md);
            font-size: 0.875rem;
            color: var(--color-text-light);
        }

        .article-date {
            display: flex;
            align-items: center;
            gap: var(--spacing-xs);
        }

        .article-read-time {
            display: flex;
            align-items: center;
            gap: var(--spacing-xs);
        }

        .article-title {
            font-size: 1.5rem;
            font-weight: 700;
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
            line-height: 1.3;
        }

        .article-excerpt {
            color: var(--color-text-light);
            margin-bottom: var(--spacing-lg);
            line-height: 1.6;
        }

        .article-tags {
            display: flex;
            flex-wrap: wrap;
            gap: var(--spacing-sm);
        }

        .tag {
            display: inline-block;
            padding: var(--spacing-xs) var(--spacing-md);
            background-color: var(--color-background-alt);
            color: var(--color-primary);
            border-radius: 9999px;
            font-size: 0.875rem;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s;
            border: 1px solid var(--color-border);
        }

        .tag:hover {
            background-color: var(--color-primary);
            color: white;
            border-color: var(--color-primary);
        }

        /* 文章详情页 */
        .article-detail {
            max-width: 800px;
            margin: 0 auto;
        }

        .article-header {
            margin-bottom: var(--spacing-2xl);
            padding-bottom: var(--spacing-xl);
            border-bottom: 1px solid var(--color-border);
        }

        .article-detail-title {
            font-size: 2.5rem;
            font-weight: 800;
            margin-bottom: var(--spacing-lg);
            line-height: 1.2;
            color: var(--color-text);
        }

        .article-detail-meta {
            display: flex;
            align-items: center;
            gap: var(--spacing-lg);
            margin-bottom: var(--spacing-lg);
            color: var(--color-text-light);
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            gap: var(--spacing-sm);
            color: var(--color-primary);
            text-decoration: none;
            font-weight: 500;
            margin-bottom: var(--spacing-xl);
            transition: color 0.2s;
        }

        .back-link:hover {
            color: var(--color-primary-dark);
        }

        /* 文章内容样式 */
        .article-content {
            font-size: 1.125rem;
            line-height: 1.8;
            color: var(--color-text);
        }

        .article-content h2 {
            font-size: 1.875rem;
            font-weight: 700;
            margin-top: var(--spacing-2xl);
            margin-bottom: var(--spacing-lg);
            color: var(--color-text);
        }

        .article-content h3 {
            font-size: 1.5rem;
            font-weight: 600;
            margin-top: var(--spacing-xl);
            margin-bottom: var(--spacing-md);
            color: var(--color-text);
        }

        .article-content p {
            margin-bottom: var(--spacing-lg);
        }

        .article-content ul, .article-content ol {
            margin-bottom: var(--spacing-lg);
            padding-left: var(--spacing-xl);
        }

        .article-content li {
            margin-bottom: var(--spacing-sm);
        }

        .article-content code {
            background-color: var(--color-code-bg);
            padding: 0.125rem 0.375rem;
            border-radius: 0.25rem;
            font-family: var(--font-mono);
            font-size: 0.9em;
            color: var(--color-primary-dark);
        }

        .article-content pre {
            background-color: var(--color-code-bg);
            padding: var(--spacing-lg);
            border-radius: 0.5rem;
            overflow-x: auto;
            margin-bottom: var(--spacing-lg);
            border: 1px solid var(--color-border);
        }

        .article-content pre code {
            background-color: transparent;
            padding: 0;
            color: var(--color-text);
            font-size: 0.875rem;
        }

        .article-content strong {
            font-weight: 600;
            color: var(--color-text);
        }

        .article-content a {
            color: var(--color-primary);
            text-decoration: none;
            border-bottom: 1px solid var(--color-primary-light);
            transition: border-color 0.2s;
        }

        .article-content a:hover {
            border-bottom-color: var(--color-primary-dark);
        }

        /* 404页面 */
        .not-found {
            text-align: center;
            padding: var(--spacing-3xl) var(--spacing-xl);
        }

        .not-found-title {
            font-size: 3rem;
            font-weight: 800;
            color: var(--color-text);
            margin-bottom: var(--spacing-md);
        }

        .not-found-text {
            font-size: 1.25rem;
            color: var(--color-text-light);
            margin-bottom: var(--spacing-xl);
        }

        /* 页面过渡动画 */
        .fade-in {
            animation: fadeIn 0.3s ease-in;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* 页脚样式 */
        .footer {
            background-color: var(--color-background-alt);
            border-top: 1px solid var(--color-border);
            padding: var(--spacing-xl) var(--spacing-xl);
            text-align: center;
            margin-top: var(--spacing-3xl);
        }

        .footer-content {
            max-width: 1200px;
            margin: 0 auto;
            color: var(--color-text-light);
            font-size: 0.875rem;
        }

        .footer-link {
            color: var(--color-text-light);
            text-decoration: none;
            transition: color 0.2s;
        }

        .footer-link:hover {
            color: var(--color-primary);
        }

        /* 响应式设计 - 移动端 */
        @media (max-width: 768px) {
            .nav-container {
                padding: var(--spacing-md) var(--spacing-lg);
            }

            .nav-title {
                font-size: 1.25rem;
            }

            .container {
                padding: var(--spacing-xl) var(--spacing-lg);
            }

            .hero-title {
                font-size: 2rem;
            }

            .hero-description {
                font-size: 1rem;
            }

            .articles-grid {
                grid-template-columns: 1fr;
                gap: var(--spacing-lg);
            }

            .article-detail-title {
                font-size: 2rem;
            }

            .article-content {
                font-size: 1rem;
            }

            .article-content h2 {
                font-size: 1.5rem;
            }

            .article-content h3 {
                font-size: 1.25rem;
            }
        }

        /* 响应式设计 - 平板 */
        @media (min-width: 768px) and (max-width: 1024px) {
            .articles-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }

        /* 响应式设计 - 桌面 */
        @media (min-width: 1024px) {
            .articles-grid {
                grid-template-columns: repeat(3, 1fr);
            }
        }
    </style>
</head>
<body>
    <!-- 导航栏 -->
    <nav class="nav" id="navigation">
        <div class="nav-container">
            <a class="nav-title" onclick="App.router.navigate('/')">AI应用开发学习</a>
            <div class="nav-links">
                <a class="nav-link" onclick="App.router.navigate('/')">首页</a>
            </div>
        </div>
    </nav>

    <!-- 主内容区域 -->
    <main id="app" class="container"></main>

    <!-- 页脚 -->
    <footer class="footer">
        <div class="footer-content">
            <a href="http://beian.miit.gov.cn/" target="_blank" rel="noopener noreferrer" class="footer-link">
                鲁ICP备2025204127号-1
            </a>
        </div>
    </footer>

    <script>
        // 文章数据
        const ARTICLES = [
            {
                id: "ai-architecture",
                title: "AI应用架构设计最佳实践",
                date: "2024-02-01",
                tags: ["AI", "架构", "部署", "最佳实践"],
                excerpt: "探讨AI应用的典型架构模式、模型部署策略、性能优化方法以及安全性和成本优化的最佳实践。",
                readTime: 10,
                content: `
                    <h2>引言</h2>
                    <p>随着AI技术的快速发展，如何构建一个高效、可靠、可扩展的AI应用架构成为了开发者面临的重要挑战。本文将深入探讨AI应用架构设计的最佳实践。</p>
                    
                    <h2>AI应用的典型架构模式</h2>
                    <h3>1. 单体架构</h3>
                    <p>适合小型应用和MVP阶段：</p>
                    <ul>
                        <li><strong>优点</strong>：开发简单，部署方便，适合快速迭代</li>
                        <li><strong>缺点</strong>：扩展性有限，难以应对高并发</li>
                        <li><strong>适用场景</strong>：原型验证、小规模应用</li>
                    </ul>
                    
                    <h3>2. 微服务架构</h3>
                    <p>将AI应用拆分为多个独立服务：</p>
                    <pre><code>API Gateway
├── 用户服务
├── 模型推理服务
├── 数据处理服务
└── 缓存服务</code></pre>
                    
                    <h3>3. Serverless架构</h3>
                    <p>利用云函数实现按需计算：</p>
                    <ul>
                        <li>自动扩缩容</li>
                        <li>按使用量付费</li>
                        <li>降低运维成本</li>
                    </ul>
                    
                    <h2>模型服务化部署</h2>
                    <h3>部署策略</h3>
                    <p>选择合适的部署方式：</p>
                    <ul>
                        <li><strong>云端部署</strong>：使用AWS SageMaker、Azure ML、Google Vertex AI</li>
                        <li><strong>边缘部署</strong>：在设备端运行轻量级模型</li>
                        <li><strong>混合部署</strong>：结合云端和边缘的优势</li>
                    </ul>
                    
                    <h3>模型版本管理</h3>
                    <pre><code>model_registry/
├── model_v1.0/
│   ├── model.pkl
│   ├── config.json
│   └── metadata.json
├── model_v1.1/
└── model_v2.0/</code></pre>
                    
                    <h2>缓存和性能优化</h2>
                    <h3>多层缓存策略</h3>
                    <p>实现高效的缓存机制：</p>
                    <ul>
                        <li><strong>应用层缓存</strong>：缓存常见查询结果</li>
                        <li><strong>模型缓存</strong>：缓存模型推理结果</li>
                        <li><strong>CDN缓存</strong>：缓存静态资源</li>
                    </ul>
                    
                    <h3>批处理优化</h3>
                    <pre><code># 批量推理示例
def batch_inference(requests, batch_size=32):
    results = []
    for i in range(0, len(requests), batch_size):
        batch = requests[i:i+batch_size]
        batch_results = model.predict(batch)
        results.extend(batch_results)
    return results</code></pre>
                    
                    <h2>安全性和隐私保护</h2>
                    <h3>数据安全</h3>
                    <ul>
                        <li>加密传输（HTTPS/TLS）</li>
                        <li>加密存储（AES-256）</li>
                        <li>访问控制（RBAC）</li>
                        <li>数据脱敏</li>
                    </ul>
                    
                    <h3>模型安全</h3>
                    <ul>
                        <li>防止模型窃取</li>
                        <li>输入验证和过滤</li>
                        <li>输出审核</li>
                        <li>对抗样本防护</li>
                    </ul>
                    
                    <h2>可观测性和监控</h2>
                    <h3>关键指标</h3>
                    <p>监控以下核心指标：</p>
                    <ul>
                        <li><strong>性能指标</strong>：延迟、吞吐量、QPS</li>
                        <li><strong>质量指标</strong>：准确率、召回率、F1分数</li>
                        <li><strong>资源指标</strong>：CPU、内存、GPU使用率</li>
                        <li><strong>业务指标</strong>：用户满意度、转化率</li>
                    </ul>
                    
                    <h3>日志和追踪</h3>
                    <pre><code>import logging

logger = logging.getLogger(__name__)

def predict_with_logging(input_data):
    logger.info(f"Prediction request: {input_data}")
    start_time = time.time()
    
    result = model.predict(input_data)
    
    latency = time.time() - start_time
    logger.info(f"Prediction completed in {latency:.3f}s")
    
    return result</code></pre>
                    
                    <h2>成本优化策略</h2>
                    <h3>计算资源优化</h3>
                    <ul>
                        <li>使用Spot实例降低成本</li>
                        <li>自动扩缩容</li>
                        <li>选择合适的实例类型</li>
                        <li>模型量化和压缩</li>
                    </ul>
                    
                    <h3>存储优化</h3>
                    <ul>
                        <li>使用对象存储（S3）存储模型</li>
                        <li>数据生命周期管理</li>
                        <li>压缩和去重</li>
                    </ul>
                    
                    <h3>API调用优化</h3>
                    <ul>
                        <li>批量请求</li>
                        <li>请求合并</li>
                        <li>缓存策略</li>
                        <li>选择合适的定价模型</li>
                    </ul>
                    
                    <h2>最佳实践总结</h2>
                    <ol>
                        <li><strong>从简单开始</strong>：先构建MVP，再逐步优化</li>
                        <li><strong>关注可观测性</strong>：建立完善的监控和告警系统</li>
                        <li><strong>自动化一切</strong>：CI/CD、测试、部署全流程自动化</li>
                        <li><strong>安全第一</strong>：在设计阶段就考虑安全性</li>
                        <li><strong>持续优化</strong>：根据监控数据不断优化架构</li>
                        <li><strong>文档化</strong>：维护清晰的架构文档和API文档</li>
                    </ol>
                    
                    <h2>总结</h2>
                    <p>构建一个优秀的AI应用架构需要综合考虑性能、可靠性、安全性和成本等多个维度。通过采用合适的架构模式、实施有效的优化策略、建立完善的监控体系，我们可以构建出高质量的AI应用。记住，架构设计是一个持续演进的过程，需要根据业务需求和技术发展不断调整和优化。</p>
                `
            },
            {
                id: "prompt-engineering",
                title: "提示工程：与AI对话的艺术",
                date: "2024-01-25",
                tags: ["AI", "提示工程", "LLM", "技巧"],
                excerpt: "掌握提示工程的核心技巧，学习如何设计高效的提示词，让AI更好地理解和执行你的意图。",
                readTime: 9,
                content: `
                    <h2>什么是提示工程</h2>
                    <p>提示工程（Prompt Engineering）是设计和优化输入提示词的过程，目的是引导大语言模型生成更准确、更有用的输出。它是与AI有效沟通的关键技能。</p>
                    
                    <h2>提示工程的基本原则</h2>
                    <h3>1. 清晰明确</h3>
                    <p>提示词应该清晰、具体、无歧义：</p>
                    <pre><code>❌ 不好的提示：
"写一篇文章"

✅ 好的提示：
"写一篇800字的技术博客文章，主题是Python异步编程，
面向有2年经验的开发者，包含代码示例和最佳实践"</code></pre>
                    
                    <h3>2. 提供上下文</h3>
                    <p>给AI足够的背景信息：</p>
                    <pre><code>你是一位资深的Python开发者，擅长异步编程。
请为初学者解释asyncio的工作原理，
使用简单的类比和代码示例。</code></pre>
                    
                    <h3>3. 指定输出格式</h3>
                    <p>明确期望的输出结构：</p>
                    <pre><code>请以JSON格式返回结果：
{
  "summary": "摘要",
  "key_points": ["要点1", "要点2"],
  "code_example": "代码示例"
}</code></pre>
                    
                    <h2>常用提示技巧</h2>
                    <h3>1. Few-shot Learning</h3>
                    <p>通过示例教会AI你想要的输出：</p>
                    <pre><code>将以下句子翻译成英文：

示例1：
输入：今天天气很好
输出：The weather is nice today

示例2：
输入：我喜欢编程
输出：I love programming

现在翻译：
输入：人工智能改变世界
输出：</code></pre>
                    
                    <h3>2. Chain-of-Thought (思维链)</h3>
                    <p>引导AI逐步思考：</p>
                    <pre><code>问题：一个商店有23个苹果，卖出了17个，
又进货了30个，现在有多少个苹果？

请一步步思考：
1. 首先计算卖出后剩余的苹果
2. 然后加上新进货的苹果
3. 得出最终答案</code></pre>
                    
                    <h3>3. Role Playing（角色扮演）</h3>
                    <p>让AI扮演特定角色：</p>
                    <pre><code>你是一位经验丰富的代码审查专家。
请审查以下Python代码，指出潜在问题和改进建议：

[代码]</code></pre>
                    
                    <h3>4. 分步指令</h3>
                    <p>将复杂任务分解为步骤：</p>
                    <pre><code>请按以下步骤分析这段代码：
1. 识别代码的主要功能
2. 找出潜在的性能问题
3. 提出优化建议
4. 给出改进后的代码</code></pre>
                    
                    <h2>提示模板设计</h2>
                    <h3>通用模板结构</h3>
                    <pre><code>[角色定义]
你是一位[专业领域]专家

[任务描述]
请[具体任务]

[约束条件]
- 要求1
- 要求2

[输出格式]
请按以下格式输出：
[格式说明]

[示例]（可选）
[示例内容]</code></pre>
                    
                    <h3>代码生成模板</h3>
                    <pre><code>作为一位资深的[编程语言]开发者，
请编写一个[功能描述]的函数。

要求：
- 使用[特定技术/库]
- 包含错误处理
- 添加详细注释
- 提供使用示例

代码风格：遵循[编码规范]</code></pre>
                    
                    <h2>提示优化方法</h2>
                    <h3>1. 迭代优化</h3>
                    <p>不断测试和改进提示词：</p>
                    <ul>
                        <li>从简单提示开始</li>
                        <li>分析输出质量</li>
                        <li>识别问题</li>
                        <li>调整提示</li>
                        <li>重新测试</li>
                    </ul>
                    
                    <h3>2. A/B测试</h3>
                    <p>比较不同提示的效果：</p>
                    <pre><code>版本A：简洁提示
版本B：详细提示
版本C：带示例的提示

对比指标：
- 准确性
- 完整性
- 响应时间</code></pre>
                    
                    <h3>3. 温度参数调整</h3>
                    <p>控制输出的随机性：</p>
                    <ul>
                        <li><strong>低温度（0-0.3）</strong>：更确定、更一致的输出</li>
                        <li><strong>中温度（0.4-0.7）</strong>：平衡创造性和准确性</li>
                        <li><strong>高温度（0.8-1.0）</strong>：更有创意、更多样化</li>
                    </ul>
                    
                    <h2>实用案例分析</h2>
                    <h3>案例1：代码调试助手</h3>
                    <pre><code>你是一位Python调试专家。我的代码出现了以下错误：

错误信息：
[错误堆栈]

相关代码：
[代码片段]

请：
1. 解释错误原因
2. 提供修复方案
3. 给出修复后的代码
4. 建议如何避免类似问题</code></pre>
                    
                    <h3>案例2：技术文档生成</h3>
                    <pre><code>请为以下API端点生成技术文档：

端点：POST /api/users
功能：创建新用户

请包含：
- 端点描述
- 请求参数（JSON格式）
- 响应格式
- 错误码说明
- 使用示例（curl命令）</code></pre>
                    
                    <h2>常见陷阱和解决方案</h2>
                    <h3>陷阱1：提示过于模糊</h3>
                    <p><strong>问题</strong>：AI输出不符合预期<br>
                    <strong>解决</strong>：增加具体细节和约束条件</p>
                    
                    <h3>陷阱2：提示过于复杂</h3>
                    <p><strong>问题</strong>：AI理解困难，输出混乱<br>
                    <strong>解决</strong>：简化提示，分步骤执行</p>
                    
                    <h3>陷阱3：缺少示例</h3>
                    <p><strong>问题</strong>：输出格式不一致<br>
                    <strong>解决</strong>：提供清晰的输出示例</p>
                    
                    <h2>高级技巧</h2>
                    <h3>1. 自我一致性</h3>
                    <p>让AI多次生成答案，选择最一致的结果：</p>
                    <pre><code>请生成3个不同的解决方案，
然后分析哪个方案最优，并说明理由。</code></pre>
                    
                    <h3>2. 反思提示</h3>
                    <p>让AI检查自己的输出：</p>
                    <pre><code>首先生成答案，然后：
1. 检查答案是否完整
2. 验证逻辑是否正确
3. 如有问题，提供改进版本</code></pre>
                    
                    <h2>总结</h2>
                    <p>提示工程是一门艺术，也是一门科学。通过掌握基本原则、常用技巧和优化方法，你可以显著提升与AI交互的效果。记住，好的提示词应该清晰、具体、有上下文，并且要不断迭代优化。随着实践的积累，你会越来越擅长设计高效的提示词。</p>
                `
            },
            {
                id: "ai-agents",
                title: "AI智能体开发实战指南",
                date: "2024-01-20",
                tags: ["AI", "智能体", "Agent", "自动化"],
                excerpt: "深入了解AI智能体的核心概念、架构设计和实现方法，学习如何构建能够自主决策和执行任务的智能系统。",
                readTime: 11,
                content: `
                    <h2>什么是AI智能体</h2>
                    <p>AI智能体（AI Agent）是一个能够感知环境、自主决策并采取行动以实现特定目标的智能系统。与传统的AI模型不同，智能体具有主动性、自主性和目标导向性。</p>
                    
                    <h3>智能体的核心特征</h3>
                    <ul>
                        <li><strong>自主性</strong>：能够在没有人类干预的情况下运行</li>
                        <li><strong>反应性</strong>：能够感知环境并及时响应</li>
                        <li><strong>主动性</strong>：能够主动采取行动实现目标</li>
                        <li><strong>社交性</strong>：能够与其他智能体或人类交互</li>
                    </ul>
                    
                    <h2>智能体的架构设计</h2>
                    <h3>感知-决策-执行循环</h3>
                    <p>智能体的基本工作流程：</p>
                    <pre><code>while True:
    # 1. 感知环境
    observation = perceive_environment()
    
    # 2. 决策
    action = decide(observation, goal)
    
    # 3. 执行动作
    result = execute(action)
    
    # 4. 更新状态
    update_state(result)
    
    # 5. 检查目标是否达成
    if goal_achieved():
        break</code></pre>
                    
                    <h3>智能体架构模式</h3>
                    <h4>1. 反应式架构</h4>
                    <p>直接将感知映射到动作，适合简单任务：</p>
                    <pre><code>def reactive_agent(perception):
    if perception == "obstacle_ahead":
        return "turn_left"
    elif perception == "goal_visible":
        return "move_forward"
    else:
        return "explore"</code></pre>
                    
                    <h4>2. 深思熟虑架构</h4>
                    <p>包含内部状态和规划能力：</p>
                    <pre><code>class DeliberativeAgent:
    def __init__(self):
        self.beliefs = {}  # 对世界的认知
        self.desires = []  # 目标
        self.intentions = []  # 计划
    
    def perceive(self, observation):
        self.update_beliefs(observation)
    
    def deliberate(self):
        # 根据信念和目标制定计划
        self.intentions = self.plan(self.beliefs, self.desires)
    
    def act(self):
        # 执行计划中的下一步
        return self.intentions.pop(0)</code></pre>
                    
                    <h4>3. 混合架构</h4>
                    <p>结合反应式和深思熟虑的优点：</p>
                    <pre><code>class HybridAgent:
    def decide(self, perception):
        # 紧急情况：反应式响应
        if self.is_urgent(perception):
            return self.reactive_response(perception)
        
        # 正常情况：深思熟虑
        return self.deliberative_response(perception)</code></pre>
                    
                    <h2>工具调用和函数调用</h2>
                    <h3>工具定义</h3>
                    <p>为智能体定义可用的工具：</p>
                    <pre><code>tools = [
    {
        "name": "search_web",
        "description": "在网络上搜索信息",
        "parameters": {
            "query": "搜索关键词"
        }
    },
    {
        "name": "calculate",
        "description": "执行数学计算",
        "parameters": {
            "expression": "数学表达式"
        }
    },
    {
        "name": "send_email",
        "description": "发送电子邮件",
        "parameters": {
            "to": "收件人",
            "subject": "主题",
            "body": "正文"
        }
    }
]</code></pre>
                    
                    <h3>工具调用实现</h3>
                    <pre><code>class ToolCallingAgent:
    def __init__(self, tools):
        self.tools = {tool["name"]: tool for tool in tools}
    
    def execute_tool(self, tool_name, parameters):
        if tool_name == "search_web":
            return self.search_web(parameters["query"])
        elif tool_name == "calculate":
            return eval(parameters["expression"])
        elif tool_name == "send_email":
            return self.send_email(**parameters)
    
    def run(self, task):
        # 1. 理解任务
        plan = self.llm.plan(task, self.tools)
        
        # 2. 执行计划
        for step in plan:
            tool_name = step["tool"]
            params = step["parameters"]
            result = self.execute_tool(tool_name, params)
            
            # 3. 根据结果调整计划
            if not self.is_successful(result):
                plan = self.replan(task, result)
        
        return result</code></pre>
                    
                    <h2>ReAct模式：推理与行动</h2>
                    <p>ReAct（Reasoning and Acting）是一种强大的智能体模式：</p>
                    <pre><code>def react_agent(task):
    thought_action_pairs = []
    
    while not task_completed():
        # Thought: 推理下一步
        thought = llm.generate(
            f"Task: {task}\n"
            f"History: {thought_action_pairs}\n"
            f"Thought:"
        )
        
        # Action: 决定行动
        action = llm.generate(
            f"Based on: {thought}\n"
            f"Action:"
        )
        
        # Observation: 执行并观察结果
        observation = execute(action)
        
        thought_action_pairs.append({
            "thought": thought,
            "action": action,
            "observation": observation
        })
    
    return extract_answer(thought_action_pairs)</code></pre>
                    
                    <h2>多智能体协作</h2>
                    <h3>协作模式</h3>
                    <h4>1. 主从模式</h4>
                    <pre><code>class MasterAgent:
    def __init__(self, worker_agents):
        self.workers = worker_agents
    
    def delegate_task(self, task):
        # 将任务分解
        subtasks = self.decompose(task)
        
        # 分配给工作智能体
        results = []
        for subtask, worker in zip(subtasks, self.workers):
            result = worker.execute(subtask)
            results.append(result)
        
        # 整合结果
        return self.integrate(results)</code></pre>
                    
                    <h4>2. 对等协作模式</h4>
                    <pre><code>class CollaborativeAgent:
    def __init__(self, agent_id, peers):
        self.id = agent_id
        self.peers = peers
        self.shared_memory = {}
    
    def collaborate(self, task):
        # 与其他智能体共享信息
        self.broadcast({"agent": self.id, "task": task})
        
        # 接收其他智能体的信息
        peer_info = self.receive_from_peers()
        
        # 基于共享信息做决策
        action = self.decide(task, peer_info)
        
        return self.execute(action)</code></pre>
                    
                    <h4>3. 竞争模式</h4>
                    <pre><code>class CompetitiveAgents:
    def run_competition(self, agents, task):
        solutions = []
        
        # 所有智能体并行工作
        for agent in agents:
            solution = agent.solve(task)
            solutions.append({
                "agent": agent,
                "solution": solution,
                "score": self.evaluate(solution)
            })
        
        # 选择最佳方案
        best = max(solutions, key=lambda x: x["score"])
        return best["solution"]</code></pre>
                    
                    <h2>实际应用案例</h2>
                    <h3>案例1：客服智能体</h3>
                    <pre><code>class CustomerServiceAgent:
    def __init__(self):
        self.knowledge_base = load_knowledge_base()
        self.conversation_history = []
    
    def handle_query(self, user_message):
        # 1. 理解用户意图
        intent = self.classify_intent(user_message)
        
        # 2. 检索相关信息
        if intent == "product_inquiry":
            info = self.knowledge_base.search(user_message)
            response = self.generate_response(info)
        
        # 3. 需要人工介入的情况
        elif intent == "complaint":
            response = "我将为您转接人工客服"
            self.escalate_to_human()
        
        # 4. 记录对话
        self.conversation_history.append({
            "user": user_message,
            "agent": response
        })
        
        return response</code></pre>
                    
                    <h3>案例2：代码审查智能体</h3>
                    <pre><code>class CodeReviewAgent:
    def review(self, code, language):
        issues = []
        
        # 1. 静态分析
        static_issues = self.static_analysis(code, language)
        issues.extend(static_issues)
        
        # 2. 代码风格检查
        style_issues = self.check_style(code, language)
        issues.extend(style_issues)
        
        # 3. 安全漏洞检测
        security_issues = self.detect_vulnerabilities(code)
        issues.extend(security_issues)
        
        # 4. 性能分析
        performance_issues = self.analyze_performance(code)
        issues.extend(performance_issues)
        
        # 5. 生成报告
        return self.generate_report(issues)</code></pre>
                    
                    <h3>案例3：研究助手智能体</h3>
                    <pre><code>class ResearchAgent:
    def research(self, topic):
        # 1. 搜索相关文献
        papers = self.search_papers(topic)
        
        # 2. 阅读和总结
        summaries = []
        for paper in papers:
            summary = self.summarize(paper)
            summaries.append(summary)
        
        # 3. 综合分析
        synthesis = self.synthesize(summaries)
        
        # 4. 生成报告
        report = self.generate_report(topic, synthesis)
        
        return report</code></pre>
                    
                    <h2>智能体的挑战和解决方案</h2>
                    <h3>挑战1：幻觉问题</h3>
                    <p><strong>解决方案</strong>：</p>
                    <ul>
                        <li>使用工具调用获取真实信息</li>
                        <li>实施事实检查机制</li>
                        <li>引入人类反馈循环</li>
                    </ul>
                    
                    <h3>挑战2：成本控制</h3>
                    <p><strong>解决方案</strong>：</p>
                    <ul>
                        <li>缓存常见查询结果</li>
                        <li>使用更小的模型处理简单任务</li>
                        <li>设置调用次数限制</li>
                    </ul>
                    
                    <h3>挑战3：安全性</h3>
                    <p><strong>解决方案</strong>：</p>
                    <ul>
                        <li>沙箱环境执行</li>
                        <li>权限控制</li>
                        <li>输入输出过滤</li>
                        <li>审计日志</li>
                    </ul>
                    
                    <h2>总结</h2>
                    <p>AI智能体代表了AI应用的新方向，它们能够自主地完成复杂任务，大大提升了AI系统的实用性。通过合理的架构设计、工具集成和多智能体协作，我们可以构建出强大而可靠的智能体系统。随着技术的发展，智能体将在更多领域发挥重要作用。</p>
                `
            },
            {
                id: "rag",
                title: "RAG技术详解：让AI更智能地检索知识",
                date: "2024-01-18",
                tags: ["AI", "RAG", "向量数据库", "检索"],
                excerpt: "深入理解检索增强生成（RAG）技术，学习如何结合向量数据库和大语言模型，构建能够准确检索和利用外部知识的AI系统。",
                readTime: 10,
                content: `
                    <h2>什么是RAG</h2>
                    <p>RAG（Retrieval-Augmented Generation，检索增强生成）是一种结合信息检索和文本生成的AI技术。它通过从外部知识库检索相关信息，然后将这些信息作为上下文提供给大语言模型，从而生成更准确、更有依据的回答。</p>
                    
                    <h3>为什么需要RAG</h3>
                    <ul>
                        <li><strong>知识时效性</strong>：LLM的训练数据有时间截止点，RAG可以提供最新信息</li>
                        <li><strong>领域专业性</strong>：可以接入特定领域的知识库</li>
                        <li><strong>减少幻觉</strong>：基于真实文档生成答案，降低虚构内容的风险</li>
                        <li><strong>可追溯性</strong>：可以引用具体的信息来源</li>
                        <li><strong>成本效益</strong>：相比微调，RAG更灵活且成本更低</li>
                    </ul>
                    
                    <h2>RAG的核心架构</h2>
                    <h3>基本流程</h3>
                    <pre><code>1. 文档处理
   ├── 文档加载
   ├── 文本分块
   └── 向量化

2. 存储
   └── 向量数据库

3. 检索
   ├── 查询向量化
   ├── 相似度搜索
   └── 结果排序

4. 生成
   ├── 构建提示词
   ├── LLM生成
   └── 返回结果</code></pre>
                    
                    <h3>实现示例</h3>
                    <pre><code>from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# 1. 加载文档
documents = load_documents("./knowledge_base")

# 2. 文本分块
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
texts = text_splitter.split_documents(documents)

# 3. 创建向量存储
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(texts, embeddings)

# 4. 创建检索链
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# 5. 查询
result = qa_chain("什么是RAG技术？")
print(result["result"])
print(result["source_documents"])</code></pre>
                    
                    <h2>向量数据库的选择</h2>
                    <h3>主流向量数据库对比</h3>
                    <table>
                        <tr>
                            <th>数据库</th>
                            <th>特点</th>
                            <th>适用场景</th>
                        </tr>
                        <tr>
                            <td>Pinecone</td>
                            <td>云原生、易用</td>
                            <td>快速原型、生产环境</td>
                        </tr>
                        <tr>
                            <td>Weaviate</td>
                            <td>开源、功能丰富</td>
                            <td>复杂查询、混合搜索</td>
                        </tr>
                        <tr>
                            <td>Chroma</td>
                            <td>轻量级、本地优先</td>
                            <td>开发测试、小规模应用</td>
                        </tr>
                        <tr>
                            <td>Milvus</td>
                            <td>高性能、可扩展</td>
                            <td>大规模生产环境</td>
                        </tr>
                    </table>
                    
                    <h3>向量数据库操作示例</h3>
                    <pre><code># 使用Chroma
import chromadb

# 创建客户端
client = chromadb.Client()

# 创建集合
collection = client.create_collection("knowledge_base")

# 添加文档
collection.add(
    documents=["文档内容1", "文档内容2"],
    metadatas=[{"source": "doc1"}, {"source": "doc2"}],
    ids=["id1", "id2"]
)

# 查询
results = collection.query(
    query_texts=["查询内容"],
    n_results=5
)

print(results)</code></pre>
                    
                    <h2>检索策略优化</h2>
                    <h3>1. 文本分块策略</h3>
                    <p>合理的分块对检索质量至关重要：</p>
                    <pre><code># 固定大小分块
splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# 递归分块（推荐）
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)

# 语义分块
splitter = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile"
)</code></pre>
                    
                    <h3>2. 混合检索</h3>
                    <p>结合关键词检索和向量检索：</p>
                    <pre><code>from langchain.retrievers import BM25Retriever, EnsembleRetriever

# BM25检索器（关键词）
bm25_retriever = BM25Retriever.from_documents(documents)

# 向量检索器
vector_retriever = vectorstore.as_retriever()

# 混合检索器
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.5, 0.5]
)</code></pre>
                    
                    <h3>3. 重排序（Reranking）</h3>
                    <p>对检索结果进行二次排序：</p>
                    <pre><code>from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank

# 创建重排序器
compressor = CohereRerank()

# 创建压缩检索器
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vector_retriever
)</code></pre>
                    
                    <h3>4. 查询改写</h3>
                    <p>优化用户查询以提高检索效果：</p>
                    <pre><code>def rewrite_query(original_query):
    prompt = f"""
    将以下用户查询改写为更适合检索的形式：
    原始查询：{original_query}
    
    改写后的查询：
    """
    return llm.generate(prompt)

# 使用
user_query = "怎么用RAG"
optimized_query = rewrite_query(user_query)
results = retriever.get_relevant_documents(optimized_query)</code></pre>
                    
                    <h2>RAG vs 微调</h2>
                    <h3>对比分析</h3>
                    <table>
                        <tr>
                            <th>维度</th>
                            <th>RAG</th>
                            <th>微调</th>
                        </tr>
                        <tr>
                            <td>知识更新</td>
                            <td>实时更新</td>
                            <td>需要重新训练</td>
                        </tr>
                        <tr>
                            <td>成本</td>
                            <td>较低</td>
                            <td>较高</td>
                        </tr>
                        <tr>
                            <td>可解释性</td>
                            <td>高（可追溯来源）</td>
                            <td>低</td>
                        </tr>
                        <tr>
                            <td>响应速度</td>
                            <td>稍慢（需检索）</td>
                            <td>快</td>
                        </tr>
                        <tr>
                            <td>适用场景</td>
                            <td>知识密集型任务</td>
                            <td>特定风格/格式</td>
                        </tr>
                    </table>
                    
                    <h3>何时选择RAG</h3>
                    <ul>
                        <li>需要频繁更新知识</li>
                        <li>需要引用具体来源</li>
                        <li>知识库较大且多样</li>
                        <li>预算有限</li>
                    </ul>
                    
                    <h3>何时选择微调</h3>
                    <ul>
                        <li>需要特定的输出风格</li>
                        <li>任务相对固定</li>
                        <li>对响应速度要求高</li>
                        <li>需要模型"内化"知识</li>
                    </ul>
                    
                    <h2>高级RAG技术</h2>
                    <h3>1. 多跳检索</h3>
                    <p>对于复杂问题，进行多轮检索：</p>
                    <pre><code>def multi_hop_retrieval(question, max_hops=3):
    context = []
    current_query = question
    
    for hop in range(max_hops):
        # 检索
        docs = retriever.get_relevant_documents(current_query)
        context.extend(docs)
        
        # 生成下一个查询
        next_query = generate_followup_query(current_query, docs)
        
        if not next_query:
            break
        
        current_query = next_query
    
    # 基于所有上下文生成答案
    answer = generate_answer(question, context)
    return answer</code></pre>
                    
                    <h3>2. 自我反思RAG</h3>
                    <p>让模型评估检索结果的相关性：</p>
                    <pre><code>def self_reflective_rag(question):
    # 初始检索
    docs = retriever.get_relevant_documents(question)
    
    # 评估相关性
    relevance_scores = []
    for doc in docs:
        score = evaluate_relevance(question, doc)
        relevance_scores.append(score)
    
    # 如果相关性不足，改写查询重新检索
    if max(relevance_scores) < threshold:
        new_query = rewrite_query(question)
        docs = retriever.get_relevant_documents(new_query)
    
    # 生成答案
    return generate_answer(question, docs)</code></pre>
                    
                    <h3>3. 图RAG</h3>
                    <p>利用知识图谱增强检索：</p>
                    <pre><code># 构建知识图谱
graph = build_knowledge_graph(documents)

# 检索时同时查询向量和图
def graph_rag(question):
    # 向量检索
    vector_results = vector_retriever.get_relevant_documents(question)
    
    # 图检索
    entities = extract_entities(question)
    graph_results = graph.query(entities)
    
    # 合并结果
    combined_context = merge_results(vector_results, graph_results)
    
    return generate_answer(question, combined_context)</code></pre>
                    
                    <h2>实践建议</h2>
                    <h3>1. 数据准备</h3>
                    <ul>
                        <li>清洗文档，移除无关内容</li>
                        <li>保持文档结构和格式一致</li>
                        <li>添加元数据（来源、日期、类别等）</li>
                    </ul>
                    
                    <h3>2. 性能优化</h3>
                    <ul>
                        <li>使用缓存减少重复检索</li>
                        <li>批量处理向量化操作</li>
                        <li>选择合适的向量维度</li>
                        <li>定期更新索引</li>
                    </ul>
                    
                    <h3>3. 质量保证</h3>
                    <ul>
                        <li>建立评估数据集</li>
                        <li>监控检索准确率</li>
                        <li>收集用户反馈</li>
                        <li>持续优化分块和检索策略</li>
                    </ul>
                    
                    <h2>总结</h2>
                    <p>RAG技术为AI应用提供了一种灵活、高效的知识增强方案。通过合理的架构设计、向量数据库选择和检索策略优化，我们可以构建出准确、可靠的RAG系统。随着技术的发展，RAG将在更多场景中发挥重要作用，成为AI应用的标准组件。</p>
                `
            },
            {
                id: "fine-tuning",
                title: "深入理解大模型微调技术",
                date: "2024-01-15",
                tags: ["AI", "微调", "LLM", "机器学习"],
                excerpt: "全面解析大语言模型微调技术，从全量微调到LoRA、QLoRA，掌握让AI模型适应特定任务的核心方法。",
                readTime: 12,
                content: `
                    <h2>什么是微调</h2>
                    <p>微调（Fine-tuning）是一种迁移学习技术，通过在预训练模型的基础上，使用特定领域或任务的数据进行进一步训练，使模型能够更好地适应目标任务。对于大语言模型（LLM）来说，微调是让通用模型变成专用模型的关键步骤。</p>
                    
                    <h3>为什么需要微调</h3>
                    <ul>
                        <li><strong>任务适配</strong>：让模型更好地理解特定领域的术语和逻辑</li>
                        <li><strong>风格定制</strong>：调整模型的输出风格和格式</li>
                        <li><strong>性能提升</strong>：在特定任务上获得更好的表现</li>
                        <li><strong>成本优化</strong>：使用更小的模型达到更好的效果</li>
                    </ul>
                    
                    <h2>微调的类型</h2>
                    <h3>1. 全量微调（Full Fine-tuning）</h3>
                    <p>更新模型的所有参数，这是最传统的微调方法。</p>
                    
                    <h4>优点</h4>
                    <ul>
                        <li>可以获得最佳性能</li>
                        <li>模型完全适应新任务</li>
                    </ul>
                    
                    <h4>缺点</h4>
                    <ul>
                        <li>需要大量计算资源</li>
                        <li>训练时间长</li>
                        <li>容易过拟合</li>
                        <li>需要存储完整模型副本</li>
                    </ul>
                    
                    <h4>代码示例</h4>
                    <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer

# 加载预训练模型
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# 准备数据
train_dataset = prepare_dataset(train_data, tokenizer)

# 配置训练参数
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
)

# 创建训练器
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

# 开始训练
trainer.train()</code></pre>
                    
                    <h3>2. LoRA（Low-Rank Adaptation）</h3>
                    <p>LoRA是一种参数高效的微调方法，通过在模型中插入低秩矩阵来实现微调，只训练这些新增的参数。</p>
                    
                    <h4>核心思想</h4>
                    <p>对于权重矩阵W，不直接更新W，而是学习两个低秩矩阵A和B：</p>
                    <pre><code>W' = W + BA
其中：
- W是原始权重（冻结）
- B的维度是 d × r
- A的维度是 r × k
- r << min(d, k)（r是秩，通常很小）</code></pre>
                    
                    <h4>优点</h4>
                    <ul>
                        <li>显著减少可训练参数（通常只有0.1%-1%）</li>
                        <li>降低显存需求</li>
                        <li>训练速度更快</li>
                        <li>可以保存多个LoRA适配器</li>
                        <li>推理时可以动态切换</li>
                    </ul>
                    
                    <h4>代码示例</h4>
                    <pre><code>from peft import LoraConfig, get_peft_model, TaskType

# 配置LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,  # 秩
    lora_alpha=32,  # 缩放因子
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"],  # 应用LoRA的模块
)

# 加载基础模型
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")

# 应用LoRA
model = get_peft_model(base_model, lora_config)

# 查看可训练参数
model.print_trainable_parameters()
# 输出：trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%

# 训练
trainer = Trainer(model=model, ...)
trainer.train()

# 保存LoRA权重（只有几MB）
model.save_pretrained("./lora_weights")</code></pre>
                    
                    <h3>3. QLoRA（Quantized LoRA）</h3>
                    <p>QLoRA在LoRA的基础上，使用4位量化来进一步降低显存需求。</p>
                    
                    <h4>关键技术</h4>
                    <ul>
                        <li><strong>4位NormalFloat量化</strong>：将模型权重量化为4位</li>
                        <li><strong>双重量化</strong>：对量化常数也进行量化</li>
                        <li><strong>分页优化器</strong>：使用CPU内存作为缓冲</li>
                    </ul>
                    
                    <h4>代码示例</h4>
                    <pre><code>from transformers import BitsAndBytesConfig
import torch

# 配置4位量化
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# 加载量化模型
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    quantization_config=bnb_config,
    device_map="auto",
)

# 配置LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# 应用LoRA
model = get_peft_model(model, lora_config)

# 现在可以在单个24GB GPU上微调70B模型！</code></pre>
                    
                    <h2>微调的实践流程</h2>
                    <h3>1. 数据准备</h3>
                    <p>高质量的数据是微调成功的关键：</p>
                    <pre><code># 数据格式示例（指令微调）
data = [
    {
        "instruction": "将以下文本翻译成英文",
        "input": "今天天气很好",
        "output": "The weather is nice today"
    },
    {
        "instruction": "总结以下文章",
        "input": "文章内容...",
        "output": "文章摘要..."
    }
]

# 数据清洗和格式化
def format_instruction(sample):
    return f"""### Instruction:
{sample['instruction']}

### Input:
{sample['input']}

### Response:
{sample['output']}"""</code></pre>
                    
                    <h3>2. 超参数选择</h3>
                    <p>关键超参数及其推荐值：</p>
                    <ul>
                        <li><strong>学习率</strong>：1e-5 到 5e-5（比预训练小）</li>
                        <li><strong>批次大小</strong>：4-32（取决于显存）</li>
                        <li><strong>训练轮数</strong>：3-10（避免过拟合）</li>
                        <li><strong>LoRA秩r</strong>：4-64（通常8-16效果好）</li>
                        <li><strong>warmup步数</strong>：总步数的10%</li>
                    </ul>
                    
                    <h3>3. 训练监控</h3>
                    <pre><code>from transformers import TrainerCallback

class MonitorCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs:
            print(f"Step: {state.global_step}")
            print(f"Loss: {logs.get('loss', 'N/A')}")
            print(f"Learning Rate: {logs.get('learning_rate', 'N/A')}")

# 使用回调
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    callbacks=[MonitorCallback()],
)</code></pre>
                    
                    <h3>4. 评估和验证</h3>
                    <pre><code># 评估函数
def evaluate_model(model, eval_dataset):
    predictions = []
    references = []
    
    for sample in eval_dataset:
        input_text = format_instruction(sample)
        pred = model.generate(input_text)
        
        predictions.append(pred)
        references.append(sample['output'])
    
    # 计算指标
    from evaluate import load
    bleu = load("bleu")
    results = bleu.compute(predictions=predictions, references=references)
    
    return results</code></pre>
                    
                    <h2>常见问题和最佳实践</h2>
                    <h3>问题1：过拟合</h3>
                    <p><strong>症状</strong>：训练损失持续下降，但验证损失上升</p>
                    <p><strong>解决方案</strong>：</p>
                    <ul>
                        <li>增加训练数据</li>
                        <li>使用数据增强</li>
                        <li>降低学习率</li>
                        <li>减少训练轮数</li>
                        <li>增加dropout</li>
                    </ul>
                    
                    <h3>问题2：灾难性遗忘</h3>
                    <p><strong>症状</strong>：模型在新任务上表现好，但原有能力下降</p>
                    <p><strong>解决方案</strong>：</p>
                    <ul>
                        <li>使用LoRA而非全量微调</li>
                        <li>混合通用数据和特定数据</li>
                        <li>使用较小的学习率</li>
                        <li>采用渐进式微调</li>
                    </ul>
                    
                    <h3>问题3：显存不足</h3>
                    <p><strong>解决方案</strong>：</p>
                    <ul>
                        <li>使用QLoRA</li>
                        <li>减小批次大小</li>
                        <li>使用梯度累积</li>
                        <li>启用梯度检查点</li>
                        <li>使用DeepSpeed或FSDP</li>
                    </ul>
                    
                    <h3>最佳实践</h3>
                    <ol>
                        <li><strong>从小规模开始</strong>：先用小数据集验证流程</li>
                        <li><strong>保存检查点</strong>：定期保存模型，防止训练中断</li>
                        <li><strong>使用验证集</strong>：及时发现过拟合</li>
                        <li><strong>记录实验</strong>：使用Weights & Biases等工具</li>
                        <li><strong>版本控制</strong>：管理数据、代码和模型版本</li>
                        <li><strong>A/B测试</strong>：对比微调前后的效果</li>
                    </ol>
                    
                    <h2>总结</h2>
                    <p>微调是让大语言模型适应特定任务的强大技术。从全量微调到LoRA、QLoRA，我们有多种选择来平衡性能和资源消耗。选择合适的微调方法，准备高质量的数据，合理设置超参数，并持续监控和评估，就能成功地微调出满足需求的模型。</p>
                `
            }
        ];

        // ArticleManager - 数据管理
        const ArticleManager = {
            getAllArticles() {
                return [...ARTICLES].sort((a, b) => new Date(b.date) - new Date(a.date));
            },
            
            getArticleById(id) {
                return ARTICLES.find(article => article.id === id) || null;
            },
            
            getArticlesByTag(tag) {
                return ARTICLES.filter(article => article.tags.includes(tag))
                    .sort((a, b) => new Date(b.date) - new Date(a.date));
            }
        };

        // HomeView - 首页视图
        const HomeView = {
            render(articles) {
                const hero = `
                    <div class="hero fade-in">
                        <h1 class="hero-title">AI应用开发学习</h1>
                        <p class="hero-description">
                            深入学习AI应用开发技术，掌握微调、RAG、智能体等核心技能
                        </p>
                    </div>
                `;
                
                const articlesGrid = `
                    <div class="articles-grid fade-in">
                        ${articles.map(article => this.renderArticleCard(article)).join('')}
                    </div>
                `;
                
                return hero + articlesGrid;
            },
            
            renderArticleCard(article) {
                return `
                    <div class="article-card" onclick="App.router.navigate('/article/${article.id}')">
                        <div class="article-meta">
                            <span class="article-date">
                                📅 ${article.date}
                            </span>
                            <span class="article-read-time">
                                ⏱️ ${article.readTime} 分钟
                            </span>
                        </div>
                        <h2 class="article-title">${article.title}</h2>
                        <p class="article-excerpt">${article.excerpt}</p>
                        <div class="article-tags">
                            ${article.tags.map(tag => `
                                <span class="tag" onclick="event.stopPropagation(); App.filterByTag('${tag}')">
                                    ${tag}
                                </span>
                            `).join('')}
                        </div>
                    </div>
                `;
            }
        };

        // ArticleView - 文章详情视图
        const ArticleView = {
            render(article) {
                if (!article) {
                    return this.render404();
                }
                
                return `
                    <div class="article-detail fade-in">
                        <a class="back-link" onclick="App.router.navigate('/')">
                            ← 返回首页
                        </a>
                        <article>
                            <header class="article-header">
                                <h1 class="article-detail-title">${article.title}</h1>
                                <div class="article-detail-meta">
                                    <span>📅 ${article.date}</span>
                                    <span>⏱️ ${article.readTime} 分钟阅读</span>
                                </div>
                                <div class="article-tags">
                                    ${article.tags.map(tag => `
                                        <span class="tag" onclick="App.router.navigate('/'); App.filterByTag('${tag}')">
                                            ${tag}
                                        </span>
                                    `).join('')}
                                </div>
                            </header>
                            <div class="article-content">
                                ${article.content}
                            </div>
                        </article>
                    </div>
                `;
            },
            
            render404() {
                return `
                    <div class="not-found fade-in">
                        <h1 class="not-found-title">404</h1>
                        <p class="not-found-text">抱歉，文章未找到</p>
                        <a class="back-link" onclick="App.router.navigate('/')">
                            ← 返回首页
                        </a>
                    </div>
                `;
            }
        };

        // Router - 路由管理
        const Router = {
            init() {
                window.addEventListener('hashchange', () => this.render());
                window.addEventListener('load', () => this.render());
            },
            
            navigate(path) {
                window.location.hash = path;
            },
            
            getCurrentPath() {
                return window.location.hash.slice(1) || '/';
            },
            
            parseRoute() {
                const path = this.getCurrentPath();
                
                if (path === '/' || path === '/home' || path === '') {
                    return { view: 'home' };
                }
                
                const articleMatch = path.match(/^\/article\/(.+)$/);
                if (articleMatch) {
                    return { view: 'article', id: articleMatch[1] };
                }
                
                return { view: 'home' };
            },
            
            render() {
                const route = this.parseRoute();
                const appElement = document.getElementById('app');
                
                if (route.view === 'home') {
                    const articles = ArticleManager.getAllArticles();
                    appElement.innerHTML = HomeView.render(articles);
                } else if (route.view === 'article') {
                    const article = ArticleManager.getArticleById(route.id);
                    appElement.innerHTML = ArticleView.render(article);
                }
                
                // 滚动到顶部
                window.scrollTo(0, 0);
            }
        };

        // App - 主应用
        const App = {
            router: Router,
            
            init() {
                this.router.init();
            },
            
            filterByTag(tag) {
                // 简单实现：导航到首页并高亮该标签的文章
                // 在实际应用中，可以实现更复杂的筛选逻辑
                console.log('Filter by tag:', tag);
            }
        };

        // 初始化应用
        document.addEventListener('DOMContentLoaded', () => {
            App.init();
        });
    </script>
</body>
</html>
